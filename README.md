# Cynaptics-Club-Inductions
This is a repo containing the code for the Cynaptics Club Induction which contains a readme file and code. Also,I could only complete task 1 due to time constraint.
# ğŸ§ Audio Classification using CNN

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)
![Librosa](https://img.shields.io/badge/Librosa-Audio%20Processing-yellow.svg)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)

A deep learning project for **environmental sound classification** using
**Log-Mel Spectrograms** and a **Convolutional Neural Network (CNN)**.\
Optimized for simplicity, reproducibility, and deployment.

------------------------------------------------------------------------

# ğŸ“š Table of Contents

1.  [Overview](#-overview)\
2.  [Features](#-features)\
3.  [Dataset Structure](#-dataset-structure)\
4.  [Model Architecture](#-model-architecture)\
5.  [Installation](#-installation)\
6.  [Training](#-training)\
7.  [Inference Example](#-inference-example)\
8.  [Future Improvements](#-future-improvements)\
9.  [License](#-license)

------------------------------------------------------------------------

# ğŸš€ Overview

This model classifies audio into **five categories**:

-   ğŸ¶ Dog Bark\
-   ğŸ”© Drilling\
-   ğŸš— Engine Idling\
-   ğŸš¨ Siren\
-   ğŸµ Street Music

The approach uses:

-   **Mel-Spectrogram extraction**
-   **CNN-based feature learning**
-   **tf.data** for fast input pipelines\
-   **Early stopping** to avoid overfitting

------------------------------------------------------------------------

# âœ¨ Features

-   ğŸµ Convert WAV audio â†’ Log-Mel Spectrogram\
-   âš¡ Fast training with `tf.data`\
-   ğŸ§  CNN with BatchNorm + Dropout\
-   ğŸ’¾ Save/Load model using `.keras`\
-   ğŸ“ˆ Includes validation tracking\
-   ğŸ”§ Highly modular and extendable

------------------------------------------------------------------------

# ğŸ“ Dataset Structure

    dataset/
     â”œâ”€â”€ dog_bark/
     â”œâ”€â”€ drilling/
     â”œâ”€â”€ engine_idling/
     â”œâ”€â”€ siren/
     â””â”€â”€ street_music/

Each folder contains `.wav` audio samples.

------------------------------------------------------------------------

# ğŸ§  Model Architecture

    Input: (400 Ã— 64 Ã— 1) Log-Mel Spectrogram
    â”‚
    â”œâ”€â”€ Conv2D(32) â†’ BatchNorm â†’ MaxPool â†’ Dropout
    â”œâ”€â”€ Conv2D(64) â†’ BatchNorm â†’ MaxPool â†’ Dropout
    â”œâ”€â”€ Conv2D(128) â†’ BatchNorm â†’ MaxPool â†’ Dropout
    â”‚
    â”œâ”€â”€ Flatten
    â”œâ”€â”€ Dense(256, ReLU) â†’ Dropout(0.5)
    â””â”€â”€ Dense(5, Softmax)

------------------------------------------------------------------------

# ğŸ›  Installation

Install everything:

``` bash
pip install tensorflow keras librosa scikit-learn numpy matplotlib
```

------------------------------------------------------------------------

# ğŸ¯ Training

Run the training script:

``` bash
python audio_classifier.py
```

The model is saved automatically at:

    C:\Python\Audio Classification_3.keras

------------------------------------------------------------------------

# ğŸ” Inference Example

``` python
from keras.models import load_model
import numpy as np
import librosa

model = load_model(r"C:\\Python\\Audio Classification_3.keras")

def predict_audio(path):
    wav, sr = librosa.load(path, sr=16000, mono=True)
    mel = librosa.feature.melspectrogram(y=wav, sr=sr, n_mels=64)
    logmel = librosa.power_to_db(mel).T
    logmel = logmel[:400, :]
    logmel = np.pad(logmel, ((0, max(0, 400 - logmel.shape[0])), (0, 0)))
    logmel = logmel[..., np.newaxis]
    pred = np.argmax(model.predict(logmel[np.newaxis]))
    return pred

print(predict_audio("test.wav"))
```

------------------------------------------------------------------------

# ğŸš§ Future Improvements

-   ğŸ”Š Audio Augmentation (noise, shift, stretch)\
-   ğŸ¼ CRNN (CNN + LSTM)\
-   ğŸ¤– Pretrained models like YAMNet, PANNs\
-   ğŸ“± Export to **TensorFlow Lite**\
-   ğŸ§ª Add test set accuracy reports

------------------------------------------------------------------------

# ğŸ“œ License

This project is released under the **MIT License**.
